{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":157270322,"sourceType":"kernelVersion"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/patrickstarrrr/dvs128gesture-cnn-0-96?scriptVersionId=161758699\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"IMB DVS128 Gesture Dataset (http://research.ibm.com/dvsgesture/) contains event-based recordings of 11 gestures made by 29 subjects under 3 different lighting conditions. A series of 11 gestures was recorded for each subject. Each gesture lasts 6 seconds. This work uses a preprocessed version of the original dataset (https://tonic.readthedocs.io/), where recordings that originally contained multiple labels have already been cut into respective samples. Also temporal precision is reduced to ms.\n\nTo avoid downloading dataset every session this kernel uses output from another kernel ([https://www.kaggle.com/code/dlarionov/create-dvs128gesture-tonic-dataset](https://www.kaggle.com/code/dlarionov/create-dvs128gesture-tonic-dataset))\n\nThis is the second part of the DVS128 Gesture Dataset exploration. It contains a straightforward solution using 2-layer CNN implemented with pytorch. The event trail for each gesture is divided into dense frames (similar to the first part). Each frame is classified as a separate image. The final class is determined by the most represented class among all frames in the trail.\n\nThe first part [https://www.kaggle.com/code/dlarionov/dvs128gesture-snntorch](https://www.kaggle.com/code/dlarionov/dvs128gesture-snntorch) uses a spiking neural network implemented with snntorch. It also contains details about dataset properties and preprocessing steps.","metadata":{}},{"cell_type":"code","source":"!pip install tonic --quiet # https://tonic.readthedocs.io/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-11T16:35:13.730134Z","iopub.execute_input":"2024-01-11T16:35:13.730645Z","iopub.status.idle":"2024-01-11T16:35:30.353289Z","shell.execute_reply.started":"2024-01-11T16:35:13.730604Z","shell.execute_reply":"2024-01-11T16:35:30.351436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom dataclasses import dataclass\nimport tonic","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:51:37.812939Z","iopub.execute_input":"2024-01-11T16:51:37.817001Z","iopub.status.idle":"2024-01-11T16:51:37.829369Z","shell.execute_reply.started":"2024-01-11T16:51:37.81693Z","shell.execute_reply":"2024-01-11T16:51:37.828146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass(frozen=True)\nclass ToOneHotTimeCoding:\n    \"\"\"    \n    encoder = ToOneHotTimeCoding(n_classes=5, n_frames=4)\n    a = encoder(np.array([3, 2, 1])) # [n] -> [n_frames n n_classes]\n    a.shape, a    \n    \"\"\"\n    n_classes: int\n    n_frames: int\n    def __call__(self, target):\n        oh = np.eye(self.n_classes)[target] # one-hot\n        res = np.array([oh for _ in range(self.n_frames)]) # stack\n        return res","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:36:29.921624Z","iopub.execute_input":"2024-01-11T16:36:29.923071Z","iopub.status.idle":"2024-01-11T16:36:29.931185Z","shell.execute_reply.started":"2024-01-11T16:36:29.923021Z","shell.execute_reply":"2024-01-11T16:36:29.929853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"debug=False # uses cache\n\ndataset_path = '/kaggle/input/create-dvs128gesture-tonic-dataset'\nw,h=64,64\nn_frames=32\n\ntransforms = tonic.transforms.Compose([\n    tonic.transforms.Denoise(filter_time=10000), # removes outlier events with inactive surrounding pixels for 10ms\n    tonic.transforms.Downsample(sensor_size=tonic.datasets.DVSGesture.sensor_size, target_size=(w,h)), # downsampling image\n    tonic.transforms.ToFrame(sensor_size=(w,h,2), n_time_bins=n_frames), # n_frames frames per trail\n])\n\ntarget_transform = ToOneHotTimeCoding(n_classes=11, n_frames=n_frames)\n\ntrain = tonic.datasets.DVSGesture(save_to=dataset_path, transform=transforms, target_transform=target_transform, train=True)\ntest = tonic.datasets.DVSGesture(save_to=dataset_path, transform=transforms, target_transform=target_transform, train=False)\n\ncached_train = train if debug else tonic.DiskCachedDataset(train, cache_path='/temp/dvsgesture/train')\ncached_test = test if debug else tonic.DiskCachedDataset(test, cache_path='/temp/dvsgesture/test')\n\nframes, labels = train[0]\nprint (tonic.datasets.DVSGesture.sensor_size, frames.shape, labels.shape)\nani = tonic.utils.plot_animation(frames)\nHTML(ani.to_jshtml())","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:54:02.336384Z","iopub.execute_input":"2024-01-11T16:54:02.336862Z","iopub.status.idle":"2024-01-11T16:54:08.376175Z","shell.execute_reply.started":"2024-01-11T16:54:02.336824Z","shell.execute_reply":"2024-01-11T16:54:08.374546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:54:23.945207Z","iopub.execute_input":"2024-01-11T16:54:23.945823Z","iopub.status.idle":"2024-01-11T16:54:23.954202Z","shell.execute_reply.started":"2024-01-11T16:54:23.945774Z","shell.execute_reply":"2024-01-11T16:54:23.952369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DVS128GestureCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n                \n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels=2, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(4),\n            nn.Dropout(0.4),\n            \n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(4),\n            nn.Dropout(0.4),\n            \n            nn.Flatten(),\n            nn.Linear(1152, 1024),\n            nn.ReLU(),            \n            nn.Dropout(0.4),\n            nn.Linear(1024, 11),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, X): # X is [batch time polarity x-pos y-pos]\n        arr = []\n        for t in range(X.shape[1]): # n_frames\n            y = self.net(X[:,t]) # [batch n_classes]\n            arr.append(y)\n        res = torch.stack(arr, dim=1) # [batch n_frames n_classes]\n        return res\n    \nmodel = DVS128GestureCNN().to(device)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters()) #, lr=0.002, betas=(0.9, 0.999))\n\n# log traces\nloss_hist = []\nacc_hist = []\ntest_acc_hist = []\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T17:11:09.446932Z","iopub.execute_input":"2024-01-11T17:11:09.447434Z","iopub.status.idle":"2024-01-11T17:11:09.479135Z","shell.execute_reply.started":"2024-01-11T17:11:09.447397Z","shell.execute_reply":"2024-01-11T17:11:09.477686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(outputs, targets): # both tensors [batch n_frames n_classes]\n    _, output_frames = torch.max(outputs, dim=2) # one-hot -> int   \n    output_preds, _ = torch.mode(output_frames, dim=1) # the most polular class    \n    _, target_frames = torch.max(targets, dim=2) # one-hot -> int\n    target_preds = target_frames[:,0] # first slice    \n    return torch.sum(output_preds == target_preds).item()/ len(output_preds)\n\ndef validate_model():\n    correct, total = 0, 0  \n    for batch, (data, targets) in enumerate(iter(test_loader)): \n        data, targets = data.to(device), targets.to(device) # [batch, n_frames, polarity, x-pos, y-pos] [batch, n_frames, n_classes] \n        outputs = model(data) # [batch, time step, n_classes]            \n        correct += accuracy(outputs, targets) * data.shape[0]\n        total += data.shape[0]\n    return correct/total","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 500\ncnt = 1\n\ntrain_loader = torch.utils.data.DataLoader(cached_train, batch_size=64, shuffle=True, drop_last=True, \n                                           collate_fn=tonic.collation.PadTensors(batch_first=True))\ntest_loader = torch.utils.data.DataLoader(cached_test, batch_size=32, shuffle=True, drop_last=True, \n                                          collate_fn=tonic.collation.PadTensors(batch_first=True))\n\nfor epoch in range(num_epochs):\n    for batch, (data, targets) in enumerate(iter(train_loader)): # [batch, time, polarity, x-pos, y-pos] [batch, time, oh]\n        data, targets = data.to(device), targets.to(device)\n        \n        outputs = model(data)\n        \n        # CrossEntropyLoss requires (N,C,d1,..dn)    \n        loss = loss_fn(outputs.permute(0,2,1), targets.permute(0,2,1)) \n        loss_hist.append(loss.item())\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()        \n        \n        acc = accuracy(outputs, targets)\n        acc_hist.append(acc)\n\n        if cnt % 100 == 0:\n            print(f\"Epoch {epoch}, Iteration {batch} \\nTrain Loss: {loss.item():.2f}\")\n            print(f\"Train Accuracy: {acc * 100:.2f}%\")\n            test_acc = validate_model()            \n            test_acc_hist.append(test_acc)\n            print(f\"Test Accuracy: {test_acc * 100:.2f}%\\n\")\n        \n        cnt+=1","metadata":{"execution":{"iopub.status.busy":"2024-01-11T17:11:23.682831Z","iopub.execute_input":"2024-01-11T17:11:23.683288Z","iopub.status.idle":"2024-01-11T17:58:33.691925Z","shell.execute_reply.started":"2024-01-11T17:11:23.683255Z","shell.execute_reply":"2024-01-11T17:58:33.6901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18,4))\n\naxes[0].plot(acc_hist)\naxes[0].set_title(\"Train Set Accuracy\")\naxes[0].set_xlabel(\"Iteration\")\naxes[0].set_ylabel(\"Accuracy\")\n\naxes[1].plot(test_acc_hist)\naxes[1].set_title(\"Test Set Accuracy\")\naxes[1].set_xlabel(\"Iteration\")\naxes[1].set_ylabel(\"Accuracy\")\n\naxes[2].plot(loss_hist)\naxes[2].set_title(\"Loss History\")\naxes[2].set_xlabel(\"Iteration\")\naxes[2].set_ylabel(\"Loss\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T17:58:33.695009Z","iopub.execute_input":"2024-01-11T17:58:33.696362Z","iopub.status.idle":"2024-01-11T17:58:34.921171Z","shell.execute_reply.started":"2024-01-11T17:58:33.696259Z","shell.execute_reply":"2024-01-11T17:58:34.919691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_model(), np.max(test_acc_hist)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T17:58:34.923238Z","iopub.execute_input":"2024-01-11T17:58:34.923695Z","iopub.status.idle":"2024-01-11T18:11:29.452615Z","shell.execute_reply.started":"2024-01-11T17:58:34.923658Z","shell.execute_reply":"2024-01-11T18:11:29.450992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}